# Requirements--
The requirements.txt file shows the requirements on the environment on GPU where the code was run

# Dataset
Unzip the dataset: Binarized MNIST, which is converted into a numpy array, is stored in Results_VAE as Archive.zip
Store the files in the working folder: copy all the .npy files to the folder "assignment3"
# Create folders
Create folders named "vae_models", "vae_svhn_models" in "assignment3"

# Q2 : Train model, calculate the ELBO and log-likelihood
Run the command: >> python3 q2.py --num_epochs=20
This will train the model and store it in "vae_models"
All the options being default, this will do all the tasks and store results in "Expt_0"
In order to get a sample, run the code: >> python3 q2_processing.py
The generated images will be stored and result folder "Expt_1" will be created

# Q3 : Train model and generate samples
Run the command: >> python3 q3_resume_mode.py --mode=train --sample_count=1000 --num_epochs=30
This will train the model and store it in "vae_svhn_models" and store results in "Expt_SVHN_0"
Then, run: >> python3 q3_processing_mode.py --mode=test --sample_count=1000 --perturb=2 --repeat_q_3_1=10 --repeat_q_3_2=100
This will store all the results in "Expt_SVHN_1". For each subquestion, there is a folder: Q_3_1, Q_3_2, Q_3_3
Q_3_1 : For samples as generated by trained model
Q_3_2 : 10 runs of perturbation experiments 
Q_3_3 : 100 runs of interpolation experiments
To compute the FID score, give path to Q_3_1 to the score_fid.py script

OPTIONAL : TAKES LONG TIME
Run the command: >> python3 q3.py --mode=train --sample_count=1000
However, this will run a lot of unnecessary methods, like computing log-likelihood of the SVHN dataset

# Final Results--
All the results, as obtained above, are stored and provided in "Results_VAE" folder in "assignment3"